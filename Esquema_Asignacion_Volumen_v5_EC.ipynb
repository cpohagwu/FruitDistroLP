{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vBcUJMDGx1xl",
        "4j14Kmcepz2C",
        "8ZyIBQqzKDpA",
        "iHuCwINLrXms",
        "pFblArfru360",
        "snllsLrTvCAz",
        "NL0nSSOkonzf",
        "vwN-ZOaGIvQv"
      ],
      "authorship_tag": "ABX9TyMhaT8QxZsi07Vp3GPsAwjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpohagwu/FruitDistroLP/blob/main/Esquema_Asignacion_Volumen_v5_EC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "ZBP6ttxhJMf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pulp --quiet"
      ],
      "metadata": {
        "id": "g7vfa4Ub8U4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef065f1-da31-4b87-c9ac-1eb356226a7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The Fruit Distribution Problem for the PuLP Modeller\n",
        "\n",
        "Authors: Collins Patrick Ohagwu, 1st Aug 2024\n",
        "\"\"\"\n",
        "\n",
        "# Import PuLP modeler functions\n",
        "import pulp"
      ],
      "metadata": {
        "id": "1IlkxDzn8W3B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "zLuIWCCi1D1u",
        "outputId": "17c61508-4f6c-4d95-bbb2-43485d651099"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6828042b1d60>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mount drive to import csv files into notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# mount drive to import csv files into notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "vBcUJMDGx1xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "hoRQ9qqr402K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all columns of pandas df\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "5UKurssdbRYV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Inspection"
      ],
      "metadata": {
        "id": "LVZkTLtL36IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory of the raw data files\n",
        "DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/Asignacion_Volumen_DOLE_LP/Data_v6'\n",
        "\n",
        "# Assign data path to a variable for easy reference\n",
        "INPUT_DEMAND = os.path.join(DATA_DIR, 'Customer rev2.csv')\n",
        "INPUT_SUPPLY = os.path.join(DATA_DIR, 'Farm Supply.csv') # This is the serve set, to make predictions on trained models\n",
        "INPUT_MATRIX = os.path.join(DATA_DIR, 'Preference Matrix.csv')"
      ],
      "metadata": {
        "id": "HLWndP0E2pGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading Functions"
      ],
      "metadata": {
        "id": "4j14Kmcepz2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_demand_nodes_by_type(df_demand, df_matrix, demand_n_matrix_col_id, demand_n_matrix_col_id_category):\n",
        "  matrix_demand_nodes_by_type_dict = df_matrix[[demand_n_matrix_col_id, demand_n_matrix_col_id_category]].set_index(demand_n_matrix_col_id).to_dict()[demand_n_matrix_col_id_category]\n",
        "  df_demand[demand_n_matrix_col_id_category] = df_demand[demand_n_matrix_col_id].map(matrix_demand_nodes_by_type_dict)\n",
        "\n",
        "  return df_demand"
      ],
      "metadata": {
        "id": "ygWvh57AOMjp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(supply_dir, demand_dir, matrix_dir):\n",
        "  \"\"\"\n",
        "  Load the dataset from the given directories.\n",
        "  \"\"\"\n",
        "  df_supply = pd.read_csv(supply_dir)\n",
        "  df_demand = pd.read_csv(demand_dir)\n",
        "  df_matrix = pd.read_csv(matrix_dir)\n",
        "\n",
        "  print('\\n==============================================================\\n')\n",
        "\n",
        "  supply_cols = df_supply.columns.to_list()\n",
        "  demand_cols = df_demand.columns.to_list()\n",
        "  matrix_cols = df_matrix.columns.to_list()\n",
        "  print(f'Supply Columns: {supply_cols}')\n",
        "  print(f'Demand Columns: {demand_cols}')\n",
        "  print(f'Matrix Columns: {matrix_cols}')\n",
        "\n",
        "  print('\\n==============================================================\\n')\n",
        "\n",
        "  return df_supply, df_demand, df_matrix"
      ],
      "metadata": {
        "id": "90wUFXf0UXMw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions\n",
        "These functions ensure the data schema (with the currect data types) and check the nodes in `supply` and `demand` sheet against those of the `preference matrix`."
      ],
      "metadata": {
        "id": "8ZyIBQqzKDpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enforce_data_schema(df_supply, supply_col_id, supply_col_val_list, df_demand, demand_col_val, df_matrix, mx_col_demand_id, mx_col_demand_id_cat):\n",
        "  \"\"\"\n",
        "  Convert data types of pandas df columns to the correct types.\n",
        "  \"\"\"\n",
        "  df_supply[supply_col_id] = df_supply[supply_col_id].astype(str)\n",
        "  supply_node_list = df_supply[supply_col_id].unique().tolist()\n",
        "  for col in supply_col_val_list:\n",
        "    df_supply[col] = df_supply[col].astype(int)\n",
        "\n",
        "  df_demand[mx_col_demand_id] = df_demand[mx_col_demand_id].astype(str)\n",
        "  demand_node_list = df_demand[mx_col_demand_id].unique().tolist()\n",
        "  df_demand[demand_col_val] = df_demand[demand_col_val].astype(int)\n",
        "  df_demand[mx_col_demand_id_cat] = df_demand[mx_col_demand_id_cat].astype(str)\n",
        "\n",
        "  df_matrix[mx_col_demand_id] = df_matrix[mx_col_demand_id].astype(str)\n",
        "  matrix_demand_node_list = df_matrix[mx_col_demand_id].unique().tolist()\n",
        "  df_matrix[mx_col_demand_id_cat] = df_matrix[mx_col_demand_id_cat].astype(str)\n",
        "\n",
        "  # Convert all columns to float\n",
        "  matrix_supply_node_list = df_matrix.drop(columns=[mx_col_demand_id, mx_col_demand_id_cat]).columns.to_list()\n",
        "  for col in matrix_supply_node_list:\n",
        "    df_matrix[col] = df_matrix[col].astype(float)\n",
        "\n",
        "  return df_supply, supply_node_list, df_demand, demand_node_list, df_matrix, matrix_supply_node_list, matrix_demand_node_list"
      ],
      "metadata": {
        "id": "LaH5AYsqOwDY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Check Nodes in Supply and Demand Sheet Against Pref. Matrix\n",
        "def check_matrix_completeness(df_supply, supply_node_list, df_demand, demand_node_list, df_matrix, matrix_supply_node_list, matrix_demand_node_list):\n",
        "  \"\"\"\n",
        "  Check that the Farm Codes in the supply sheet are present in the preference matrix.\n",
        "  Also check that the SKU's in the demand sheet are present in the preference matrix.\n",
        "  Else, remove any SKU's or Farm Codes that are not present in the preference matrix.\n",
        "  \"\"\"\n",
        "  ################################################\n",
        "  # SUPPLY SHEET\n",
        "  # Check that the Farm Codes in the supply sheet are present in the preference matrix\n",
        "  supply_node_not_in_matrix = [node for node in supply_node_list if node not in matrix_supply_node_list]\n",
        "  matrix_supply_node_not_in_supply = [node for node in matrix_supply_node_list if node not in supply_node_list]\n",
        "\n",
        "  # Instanciate the log messages\n",
        "  log_messages = []\n",
        "  if len(supply_node_not_in_matrix) > 0 | len(matrix_supply_node_not_in_supply) > 0:\n",
        "\n",
        "    log_messages.append(f'# Farm Codes in the supply sheet but not in the preference matrix: {len(supply_node_not_in_matrix)} \\n Node List: {supply_node_not_in_matrix}')\n",
        "    log_messages.append('WARNING! Values from the Supply Nodes above will not be assigned to any Demand Node.\\n')\n",
        "    log_messages.append(f'# Farm Codes in the preference matrix but not in the supply sheet: {len(matrix_supply_node_not_in_supply)} \\n Node List: {matrix_supply_node_not_in_supply}')\n",
        "    log_messages.append('==============================================================')\n",
        "    # Exlude nodes from supply sheet\n",
        "    df_supply = df_supply[~df_supply['Code'].isin(supply_node_not_in_matrix)]\n",
        "    # Exclude nodes from the preference matrix\n",
        "    df_matrix = df_matrix.drop(columns=matrix_supply_node_not_in_supply)\n",
        "\n",
        "  else:\n",
        "    log_messages.append('All Farm Codes in the Supply Sheet are present in the Preference Matrix.')\n",
        "    log_messages.append('==============================================================')\n",
        "\n",
        "  ################################################\n",
        "  # DEMAND SHEET\n",
        "  # Check that the SKU's in the demand sheet are present in the preference matrix\n",
        "  demand_node_not_in_matrix = [node for node in demand_node_list if node not in matrix_demand_node_list]\n",
        "  matrix_demand_node_not_in_demand = [node for node in matrix_demand_node_list if node not in demand_node_list]\n",
        "\n",
        "  if len(demand_node_not_in_matrix) > 0 | len(matrix_demand_node_not_in_demand) > 0:\n",
        "\n",
        "    log_messages.append(f'# SKU\\'s in the demand sheet but not in the preference matrix: {len(demand_node_not_in_matrix)} \\n Node List: {demand_node_not_in_matrix}')\n",
        "    log_messages.append('WARNING! Values in the Demand Nodes above will not be satisfied from any Supply Node.\\n')\n",
        "    log_messages.append(f'# SKU\\'s in the preference matrix but not in the demand sheet: {len(matrix_demand_node_not_in_demand)} \\n Node List: {matrix_demand_node_not_in_demand}')\n",
        "    log_messages.append('==============================================================')\n",
        "    # Exlude nodes from demand sheet\n",
        "    df_demand = df_demand[~df_demand['SKU'].isin(demand_node_not_in_matrix)]\n",
        "    # Exclude nodes from the preference matrix\n",
        "    df_matrix = df_matrix.drop(columns=matrix_demand_node_not_in_demand)\n",
        "\n",
        "  else:\n",
        "    log_messages.append('All SKU\\'s in the Demand Sheet are present in the Preference Matrix.')\n",
        "    log_messages.append('==============================================================')\n",
        "\n",
        "  return log_messages, df_supply, df_demand, df_matrix"
      ],
      "metadata": {
        "id": "MMV3KhNXGtCo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Data Loading and Preprocess Functions\n",
        "\n",
        "The following variables are defined based on domain knowledge of the dataset."
      ],
      "metadata": {
        "id": "iHuCwINLrXms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supply_col_id = 'Code' # This is the farm code from production.\n",
        "supply_col_val_list = ['Regular', 'Pre Weighted'] # Also the fruit type:  `Regular`, `Pre Pesado`, etc.\n",
        "\n",
        "demand_col_val = 'Volume' # This is the volume requested per SKU.\n",
        "\n",
        "demand_n_matrix_col_id = 'SKU' # This is the SKU from logistics.\n",
        "demand_n_matrix_col_id_category = 'Type' # This is the sub classification found on the `supply_col_val_list`."
      ],
      "metadata": {
        "id": "3_G3maxVQlIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_supply_raw_all, df_demand_raw_all, df_matrix_raw = load_dataset(INPUT_SUPPLY, INPUT_DEMAND, INPUT_MATRIX)\n",
        "\n",
        "df_demand_raw_all = assign_demand_nodes_by_type(df_demand_raw_all, df_matrix_raw, demand_n_matrix_col_id, demand_n_matrix_col_id_category)"
      ],
      "metadata": {
        "id": "1BMwXnwgP7te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter for a Single Week's Data.\n",
        "\n",
        "Normally, this should node be needed since the dataset is refreshed each week."
      ],
      "metadata": {
        "id": "5sWYxqhmFfxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter fruit distribution by week and drop `Week` column\n",
        "WEEK = 'WK30'\n",
        "df_supply_raw = df_supply_raw_all[df_supply_raw_all['Week'] == WEEK].drop(columns=['Week']).fillna(0)\n",
        "df_demand_raw = df_demand_raw_all[df_demand_raw_all['Week'] == WEEK].drop(columns=['Week']).fillna(0)\n",
        "\n",
        "print(f'Supply: {df_supply_raw.shape}')\n",
        "print(f'Demand: {df_demand_raw_all.shape}')"
      ],
      "metadata": {
        "id": "f0-xoIpHFdBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Preprocessing Functions"
      ],
      "metadata": {
        "id": "Cl4oBj3lJ2Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data types of pandas df columns to the correct types.\n",
        "df_supply_fx, supply_node_list, df_demand_fx, demand_node_list, df_matrix_fx, matrix_supply_node_list, matrix_demand_node_list = enforce_data_schema(df_supply_raw, supply_col_id, supply_col_val_list, df_demand_raw, demand_col_val, df_matrix_raw, demand_n_matrix_col_id, demand_n_matrix_col_id_category)\n",
        "# Check Nodes in Supply and Demand Sheet Against Pref. Matrix\n",
        "logs, df_supply_fx_mx, df_demand_fx_mx, df_matrix_fx_mx = check_matrix_completeness(df_supply_fx, supply_node_list, df_demand_fx, demand_node_list, df_matrix_fx, matrix_supply_node_list, matrix_demand_node_list)"
      ],
      "metadata": {
        "id": "Q7_Lq0MyJlQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs"
      ],
      "metadata": {
        "id": "VFzfO3V5pcmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LP Model Functions\n",
        "Functions to handle the creation of linear programming **descision variables, update the variables to include dummy demand nodes (to handle excess supply) and dummy supply nodes (to handle excess demand), create and solve the LP problem, and interpret and output the results as pandas dataframes**."
      ],
      "metadata": {
        "id": "xS_nRg-B5bLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descision Variables"
      ],
      "metadata": {
        "id": "pFblArfru360"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pulp\n",
        "\n",
        "def generate_variable_dictionaries(df_supply: pd.DataFrame, supply_id: str, supply_val: str, df_demand: pd.DataFrame, demand_id: str, demand_val: str, df_matrix: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Generate dictionaries for decision variables\n",
        "  Args:\n",
        "    df_supply: df with supply data\n",
        "    supply_id: column name for supply id\n",
        "    supply_val: column name for supply value\n",
        "    df_demand: df with demand data\n",
        "    demand_id:  column name for demand id\n",
        "    demand_val: column name for demand value\n",
        "    df_matrix:  df for the preference matrix per supply (row indeces) and demand (column names)\n",
        "\n",
        "    Returns:\n",
        "    supply_dict: dictionary for the number of units of supply for each supply node\n",
        "    demand_dict: dictionary for the number of units of demand for each demand node\n",
        "    preference_dict: dictionary for the preference matrix\n",
        "  \"\"\"\n",
        "  # Initialize log messages\n",
        "  log_messages = []\n",
        "  log_messages.append(f'SOLVING FOR `{supply_val}` TYPE')\n",
        "  log_messages.append('\\n==============================================================\\n')\n",
        "\n",
        "  # Dictionary for the number of units of supply for each supply node\n",
        "  supply_dict = {row[supply_id]: row[supply_val] for _, row in df_supply.iterrows()}\n",
        "\n",
        "  # Dictionary for the number of units of demand for each demand node\n",
        "  demand_dict = {row[demand_id]: row[demand_val] for _, row in df_demand.iterrows()}\n",
        "\n",
        "  # Dictionary of dictionary for the preference matrix\n",
        "  df_matrix = df_matrix.transpose() # Transpose and use the first row as header\n",
        "  new_header = df_matrix.iloc[0] #grab the first row for the header\n",
        "  df_matrix = df_matrix[1:] #take the data less the header row\n",
        "  df_matrix.columns = new_header #set the header row as the df header\n",
        "  preference_dict = df_matrix.to_dict('index')\n",
        "\n",
        "  # Print the number of supply and demand nodes\n",
        "  log_messages.append('Before Dummy Update')\n",
        "  log_messages.append(f'# Supply Nodes: {len(supply_dict)}')\n",
        "  log_messages.append(f'# Demand Nodes: {len(demand_dict)}')\n",
        "  log_messages.append(f'Preference df Shape: {df_matrix.shape[0]} Supply Rows | {df_matrix.shape[1]} Demand Columns')\n",
        "\n",
        "  # Calculate the total supply and demand\n",
        "  log_messages.append(f'Total Supply: {int(sum(supply_dict.values())):,}')\n",
        "  log_messages.append(f'Total Demand: {int(sum(demand_dict.values())):,}')\n",
        "  log_messages.append('\\n==============================================================\\n')\n",
        "\n",
        "  return log_messages, supply_dict, demand_dict, preference_dict"
      ],
      "metadata": {
        "id": "LBcdoC5VFrlm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_vd_for_dummy_nodes(log_messages, supply_dict: dict, demand_dict: dict, preference_dict: dict):\n",
        "  \"\"\"\n",
        "  Update supply and demand dictionaries with dummy nodes for excess or deficit supply conditions\n",
        "  \"\"\"\n",
        "  # Calculate the total supply and demand\n",
        "  total_supply = sum(supply_dict.values())\n",
        "  total_demand = sum(demand_dict.values())\n",
        "\n",
        "  # Update demand nodes with Dummy nodes\n",
        "  dummy_demand_node = 'DummyDemandNode'\n",
        "  supply_excess = total_supply - total_demand\n",
        "\n",
        "  # Update demand dict with Dummy EXCESS demand (to cover excess supply)\n",
        "  if supply_excess > 0:\n",
        "    demand_dict[dummy_demand_node] = abs(supply_excess)\n",
        "    log_messages.append(f'Dummy Excess Demand: {abs(supply_excess):,}')\n",
        "\n",
        "  # Update supply nodes with Dummy nodes\n",
        "  dummy_supply_node = 'DummySupplyNode'\n",
        "\n",
        "  # Update supply dict with Dummy EXCESS supply (to cover excess demand)\n",
        "  if supply_excess < 0:\n",
        "    supply_dict[dummy_supply_node] = abs(supply_excess)\n",
        "    log_messages.append(f'Dummy Excess Supply: {abs(supply_excess):,}')\n",
        "\n",
        "  # Update preference dict with Dummy Supply and Demand nodes\n",
        "  preference_df = pd.DataFrame(preference_dict).T # as pandas dataframe for updates\n",
        "  # Add new dummy supply to the preference matrix if present in the supply dict keys (nodes)\n",
        "  if dummy_supply_node in supply_dict.keys():\n",
        "    preference_df.loc[dummy_supply_node] = 0\n",
        "  # Add new dummy demand to the preference matrix if present in the demand dict keys (nodes)\n",
        "  if dummy_demand_node in demand_dict.keys():\n",
        "    preference_df[dummy_demand_node] = 0\n",
        "  # Convert back to dictionary\n",
        "  preference_dict = preference_df.to_dict('index')\n",
        "\n",
        "  # Print the number of supply and demand nodes after updating\n",
        "  log_messages.append('After Dummy Update')\n",
        "  log_messages.append(f'# Supply Nodes: {len(supply_dict)}')\n",
        "  log_messages.append(f'# Demand Nodes: {len(demand_dict)}')\n",
        "  log_messages.append(f'Preference df Shape: {preference_df.shape[0]} Supply Rows | {preference_df.shape[1]} Demand Columns')\n",
        "\n",
        "  # Calculate the total supply and demand after updating\n",
        "  log_messages.append(f'Total Supply: {int(sum(supply_dict.values())):,}')\n",
        "  log_messages.append(f'Total Demand: {int(sum(demand_dict.values())):,}')\n",
        "  log_messages.append('\\n==============================================================\\n')\n",
        "\n",
        "  return log_messages, supply_dict, demand_dict, preference_dict"
      ],
      "metadata": {
        "id": "vMt4Ib7aJnvy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statements/ Objective Functions/ Constraints"
      ],
      "metadata": {
        "id": "snllsLrTvCAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_solve_lp_problem(log_messages, supply_dict: dict, demand_dict: dict, preference_dict: dict):\n",
        "\n",
        "  \"\"\"\n",
        "  Create and solve the LP problem\n",
        "  \"\"\"\n",
        "\n",
        "  # Creates the 'prob' variable to contain the problem data\n",
        "  prob = pulp.LpProblem(\"Fruit_Distribution_Problem\", pulp.LpMaximize)\n",
        "\n",
        "  # Creates a list of tuples containing all the possible routes for transport\n",
        "  routes_tup = [(s, d) for s in supply_dict.keys() for d in demand_dict.keys()]\n",
        "\n",
        "  # A dictionary called 'Vars' is created to contain the referenced variables(the routes)\n",
        "  vars = pulp.LpVariable.dicts(\"Route\", (supply_dict.keys(), demand_dict.keys()), 0, None, pulp.LpInteger)\n",
        "\n",
        "  # The objective function is added to 'prob' first\n",
        "  prob += (\n",
        "      pulp.lpSum([vars[s][d] * preference_dict[s][d] for (s, d) in routes_tup]),\n",
        "      \"Sum_of_Preference_Matrix\",\n",
        "      )\n",
        "\n",
        "  # The supply maximum constraints are added to prob for each supply node\n",
        "  for s in supply_dict.keys():\n",
        "      prob += (\n",
        "          pulp.lpSum([vars[s][d] for d in demand_dict.keys()]) <= supply_dict[s],\n",
        "          f\"Sum_of_Fruits_out_of_Farm_{s}\",\n",
        "      )\n",
        "\n",
        "  # The demand minimum constraints are added to prob for each demand node\n",
        "  for d in demand_dict.keys():\n",
        "      prob += (\n",
        "          pulp.lpSum([vars[s][d] for s in supply_dict.keys()]) >= demand_dict[d],\n",
        "          f\"Sum_of_Fruits_into_SKU_{d}\",\n",
        "      )\n",
        "\n",
        "  # The problem is solved using PuLP's choice of Solver\n",
        "  prob.solve() # takes no argument, uses default solver\n",
        "\n",
        "  # The status of the solution is printed to the screen\n",
        "  log_messages.append(f\"Solution Status: {pulp.LpStatus[prob.status]}\")\n",
        "  log_messages.append('\\n==============================================================\\n')\n",
        "\n",
        "  return log_messages, prob"
      ],
      "metadata": {
        "id": "5ii1bWm5LpFS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(prob: pulp.LpProblem):\n",
        "  \"\"\"\n",
        "  Get results in matrix and direct form\n",
        "  \"\"\"\n",
        "\n",
        "  # Process matrix results, (all values in a grid form)\n",
        "  results = {}\n",
        "  # Each of the variables is printed with it's resolved optimum value\n",
        "  for route in prob.variables():\n",
        "    # print(f'{route.name}: {route.varValue}')\n",
        "    route_name = route.name.split(\"_\")\n",
        "    supply = route_name[1]\n",
        "    demand = route_name[2]\n",
        "\n",
        "    if supply not in results:\n",
        "      results[supply] = {}\n",
        "\n",
        "    results[supply][demand] = int(route.varValue)\n",
        "\n",
        "  df_matrix_results = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "  # Process direct results, (only none zero values)\n",
        "  results = []\n",
        "  # Each of the variables is printed with it's resolved optimum value\n",
        "  for route in prob.variables():\n",
        "    # Only non zero values\n",
        "    if route.varValue > 0:\n",
        "\n",
        "      route_name = route.name.split(\"_\")\n",
        "      supply = route_name[1]\n",
        "      demand = route_name[2]\n",
        "\n",
        "      results_route = []\n",
        "      results_route.append(supply)\n",
        "      results_route.append(demand)\n",
        "      results_route.append(int(route.varValue))\n",
        "\n",
        "      results.append(results_route)\n",
        "\n",
        "    df_direct_results = pd.DataFrame(results, columns=['Supply (Farm Codes)', 'Demand (SKU)', 'Units (Boxes)'])\n",
        "\n",
        "  return df_matrix_results, df_direct_results"
      ],
      "metadata": {
        "id": "KhwlmHWbNqi6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call Linear Programming Functions"
      ],
      "metadata": {
        "id": "NL0nSSOkonzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lp_model(df_supply: pd.DataFrame, supply_id: str, supply_col_val_list: list, df_demand: pd.DataFrame, demand_id: str, demand_val: str, df_matrix: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Run the LP model\n",
        "  \"\"\"\n",
        "  # Initialize model list\n",
        "  models = []\n",
        "  # Initialize log messages\n",
        "  log_messages = []\n",
        "  # Iterate through each supply value\n",
        "  for supply_val in supply_col_val_list:\n",
        "    # Generate dictionaries of decision variables\n",
        "    logs, supply_dict, demand_dict, preference_dict = generate_variable_dictionaries(df_supply, supply_id, supply_val, df_demand, demand_id, demand_val, df_matrix)\n",
        "    # Update supply and demand dictionaries with dummy nodes for excess or deficit supply conditions\n",
        "    logs, supply_dict, demand_dict, preference_dict = update_vd_for_dummy_nodes(logs, supply_dict, demand_dict, preference_dict)\n",
        "    # Create and solve lp problem\n",
        "    logs, model = create_and_solve_lp_problem(logs, supply_dict, demand_dict, preference_dict)\n",
        "    # Append model to list\n",
        "    models.append(model)\n",
        "    # Append logs to list\n",
        "    log_messages.append(logs)\n",
        "  return log_messages, models"
      ],
      "metadata": {
        "id": "IJffE11XdjR-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_model_results(models: list):\n",
        "  \"\"\"\n",
        "  Process the model results\n",
        "  \"\"\"\n",
        "  # Initialize results list\n",
        "  df_matrix_results = []\n",
        "  df_direct_results = []\n",
        "\n",
        "  # Initialize log messages\n",
        "  log_messages = []\n",
        "  log_messages.append('==============================================================')\n",
        "  log_messages.append('PROCESSING MODEL RESULTS')\n",
        "  log_messages.append('==============================================================')\n",
        "\n",
        "  count = 1\n",
        "\n",
        "  # Iterate through each model\n",
        "  for model in models:\n",
        "    # Get results in matrix and direct form\n",
        "    if model.status == 1:\n",
        "      df_matrix_result, df_direct_result = get_results(model)\n",
        "      df_matrix_results.append(df_matrix_result)\n",
        "      df_direct_results.append(df_direct_result)\n",
        "      log_messages.append(f'Problem {count} solved!')\n",
        "      # Write lp problem to file\n",
        "      model_name = f\"Fruit_Distribution_Problem_{count}.lp\"\n",
        "      model.writeLP(model_name)\n",
        "      log_messages.append(f\"The problem is written to {model_name}\")\n",
        "      log_messages.append('==============================================================')\n",
        "\n",
        "    else:\n",
        "      log_messages.append('Problem not solved!')\n",
        "      log_messages.append('==============================================================')\n",
        "\n",
        "    df_matrix_results_concat = pd.concat(df_matrix_results, axis=0)\n",
        "    df_direct_results_concat = pd.concat(df_direct_results, axis=0)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "  return log_messages, df_matrix_results_concat, df_direct_results_concat"
      ],
      "metadata": {
        "id": "uLA1DQJ3m5uO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run LP Functions"
      ],
      "metadata": {
        "id": "vwN-ZOaGIvQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_list(nested_list):\n",
        "  # Check if the list is nested\n",
        "  if any(isinstance(i, list) for i in nested_list):\n",
        "    # Flatten a nested list\n",
        "    return [item for sublist in nested_list for item in sublist]\n",
        "  else:\n",
        "    return nested_list"
      ],
      "metadata": {
        "id": "BuuHsu9joWMe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_logs, models = run_lp_model(df_supply_fx_mx, supply_col_id, supply_col_val_list, df_demand_fx_mx, demand_n_matrix_col_id, demand_col_val, df_matrix_fx)\n",
        "results_logs, df_matrix_results, df_direct_results = process_model_results(models)\n",
        "\n",
        "# Save the processed CSV for download\n",
        "df_matrix_results.to_csv('df_matrix_results.csv', index=False)\n",
        "df_direct_results.to_csv('df_direct_results.csv', index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Pevokyyfxmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "edc7301d-9adf-4cfe-916f-957031af012e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_supply_fx_mx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6726cda01822>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_lp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_supply_fx_mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupply_col_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupply_col_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_demand_fx_mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand_n_matrix_col_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand_col_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_matrix_fx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_matrix_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_direct_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save the processed CSV for download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_matrix_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_matrix_results.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_supply_fx_mx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs = []\n",
        "logs.append(flatten_list(mod_logs))\n",
        "logs.append(flatten_list(results_logs))\n",
        "logs = flatten_list(logs)"
      ],
      "metadata": {
        "id": "2SkO1qkCvg0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results Inspection"
      ],
      "metadata": {
        "id": "5MTiLKMqu64E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_matrix_results.to_csv('df_matrix_results.csv', index=False)\n",
        "df_direct_results.to_csv('df_direct_results.csv', index=False)"
      ],
      "metadata": {
        "id": "LdJy7Da11K9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_matrix_results.head(10)"
      ],
      "metadata": {
        "id": "WBrkX-cpk2YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_direct_results.head(10)"
      ],
      "metadata": {
        "id": "nnZ2I87wDnWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio App"
      ],
      "metadata": {
        "id": "NADXUwQ4U6si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "xlKWrbGDUVL2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405831f9-c609-46c6-f0b0-b580a893f6ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "g9h4LNynVWHN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_list(list_items):\n",
        "    # Convert list to a string with each item on a new line\n",
        "    list_string = \"\\n\".join(list_items)\n",
        "    return list_string"
      ],
      "metadata": {
        "id": "P1pm4lRUwgw1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In run_lp_model, use the state variables\n",
        "def gr_run_lp_model(df_supply, supply_col_id, supply_col_val_list, df_demand, demand_n_matrix_col_id, demand_col_val, df_matrix):\n",
        "\n",
        "    import ast\n",
        "    # Convert string to list\n",
        "    supply_col_val_list = ast.literal_eval(supply_col_val_list)\n",
        "\n",
        "    # Run LP model\n",
        "    mod_logs, models = run_lp_model(df_supply, supply_col_id, supply_col_val_list, df_demand, demand_n_matrix_col_id, demand_col_val, df_matrix)\n",
        "\n",
        "    # Process model results\n",
        "    results_logs, df_matrix_results, df_direct_results = process_model_results(models)\n",
        "\n",
        "    # Flatten logs\n",
        "    logs = []\n",
        "    logs.append(flatten_list(mod_logs))\n",
        "    logs.append(flatten_list(results_logs))\n",
        "    logs = flatten_list(logs)\n",
        "\n",
        "    # Save the processed CSV for download\n",
        "    matrix_results_csv_path = '/matrix_results.csv'\n",
        "    direct_results_csv_path = '/direct_results.csv'\n",
        "    df_matrix_results.to_csv(matrix_results_csv_path, index=True)\n",
        "    df_direct_results.to_csv(direct_results_csv_path, index=True)\n",
        "\n",
        "    return display_list(logs), df_direct_results, matrix_results_csv_path, direct_results_csv_path"
      ],
      "metadata": {
        "id": "5Yr2lnG-rkg0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gr_process_files(supply_file, demand_file, matrix_file, supply_col_id, supply_col_val_list):\n",
        "  \"\"\"\n",
        "  Process the files\n",
        "  \"\"\"\n",
        "\n",
        "  supply_col_id = 'Code' # This is the farm code from production.\n",
        "  supply_col_val_list = ['Regular', 'Pre Weighted'] # Also the fruit type:  `Regular`, `Pre Pesado`, etc.\n",
        "\n",
        "  demand_n_matrix_col_id = 'SKU' # This is the SKU from logistics.\n",
        "  demand_col_val = 'Volume' # This is the volume requested per SKU.\n",
        "  demand_n_matrix_col_id_category = 'Type' # This is the sub classification found on the `supply_col_val_list`.\n",
        "\n",
        "\n",
        "  # Load the files\n",
        "  df_supply, df_demand, df_matrix = load_dataset(supply_file.name, demand_file.name, matrix_file.name)\n",
        "  df_demand = assign_demand_nodes_by_type(df_demand, df_matrix, demand_n_matrix_col_id, demand_n_matrix_col_id_category)\n",
        "\n",
        "  # Filter fruit distribution by week and drop `Week` column\n",
        "  WEEK = 'WK29'\n",
        "  df_supply = df_supply_raw_all[df_supply['Week'] == WEEK].drop(columns=['Week']).fillna(0)\n",
        "  df_demand = df_demand[df_demand['Week'] == WEEK].drop(columns=['Week']).fillna(0)\n",
        "\n",
        "  df_supply, supply_node_list, df_demand, demand_node_list, df_matrix, matrix_supply_node_list, matrix_demand_node_list = enforce_data_schema(df_supply, supply_col_id, supply_col_val_list, df_demand, demand_col_val, df_matrix, demand_n_matrix_col_id, demand_n_matrix_col_id_category)\n",
        "  logs, df_supply, df_demand, df_matrix = check_matrix_completeness(df_supply, supply_node_list, df_demand, demand_node_list, df_matrix, matrix_supply_node_list, matrix_demand_node_list)\n",
        "\n",
        "  return display_list(logs), supply_col_id,  supply_col_val_list, df_supply, demand_n_matrix_col_id, demand_col_val, demand_n_matrix_col_id_category, df_demand, df_matrix\n"
      ],
      "metadata": {
        "id": "LmW6pvYbVmDG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "with gr.Blocks() as app:\n",
        "    # Logo\n",
        "    img_url=\"https://upload.wikimedia.org/wikipedia/en/thumb/d/d5/Dole_Foods_Logo_Green_Leaf.svg/1280px-Dole_Foods_Logo_Green_Leaf.svg.png\"\n",
        "    gr.Image(value=img_url, width=100, height=100)\n",
        "\n",
        "    gr.Markdown(\"# Fruit Distribution App\")\n",
        "\n",
        "    ###############################################\n",
        "    gr.Markdown(\"## File Procesor\")\n",
        "    # CSV File Uploads\n",
        "    supply = gr.File(label=\"Upload Supply CSV File\")\n",
        "    demand = gr.File(label=\"Upload Demand CSV File\")\n",
        "    matrix = gr.File(label=\"Upload Matrix CSV File\")\n",
        "\n",
        "    # Process Button\n",
        "    process_button = gr.Button(\"Process Files\")\n",
        "\n",
        "    # Outputs\n",
        "    gr_logs = gr.Textbox(label=\"Data Processing Logs\")\n",
        "    gr_supply_col_id = gr.Textbox(label=\"Supply Column ID\")\n",
        "    gr_supply_col_val_list = gr.Textbox(label=\"Supply Column Value List\")\n",
        "    gr_df_supply = gr.Dataframe(label=\"Supply Dataframe\")\n",
        "\n",
        "    gr_demand_n_matrix_col_id = gr.Textbox(label=\"Demand N Matrix Column ID\")\n",
        "    gr_demand_col_val = gr.Textbox(label=\"Demand Column Value\")\n",
        "    gr_demand_n_matrix_col_id_category = gr.Textbox(label=\"Demand N Matrix Column ID Category\")\n",
        "    gr_df_demand = gr.Dataframe(label=\"Demand Dataframe\")\n",
        "\n",
        "    gr_df_matrix = gr.Dataframe(label=\"Matrix Dataframe\")\n",
        "\n",
        "    # Link function to button\n",
        "    process_button.click(fn=gr_process_files,\n",
        "                         inputs=[supply, demand, matrix],\n",
        "                         outputs=[gr_logs,\n",
        "                                  gr_supply_col_id, gr_supply_col_val_list, gr_df_supply,\n",
        "                                  gr_demand_n_matrix_col_id, gr_demand_col_val, gr_demand_n_matrix_col_id_category, gr_df_demand,\n",
        "                                  gr_df_matrix])\n",
        "\n",
        "\n",
        "    ###############################################\n",
        "    gr.Markdown(\"## Model Builder\")\n",
        "    # Run Model Button\n",
        "    run_model_button = gr.Button(\"Run Model\")\n",
        "\n",
        "    # Outputs\n",
        "    out_logs = gr.Textbox(label=\"Model Building Logs\")\n",
        "    out_df_direct_results = gr.Dataframe(label=\"Direct Results Dataframe\")\n",
        "\n",
        "    download_matrix_results = gr.File(label=\"Download Matrix Results\")\n",
        "    download_direct_results = gr.File(label=\"Download Direct Results\")\n",
        "\n",
        "    # Link function to button\n",
        "    run_model_button.click(fn=gr_run_lp_model,\n",
        "                           inputs=[gr_df_supply, gr_supply_col_id, gr_supply_col_val_list,\n",
        "                                   gr_df_demand, gr_demand_n_matrix_col_id, gr_demand_col_val,\n",
        "                                   gr_df_matrix],\n",
        "                           outputs=[out_logs,\n",
        "                                    out_df_direct_results,\n",
        "                                    download_matrix_results, download_direct_results])\n",
        "\n",
        "# Launch the app\n",
        "app.launch()"
      ],
      "metadata": {
        "id": "AaRKZpQQ1Htz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "f13a4286-f1c6-4e47-e5eb-f6eeb95bcaa6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1002: UserWarning: Expected 5 arguments for function <function gr_process_files at 0x78614c648ca0>, received 3.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1006: UserWarning: Expected at least 5 arguments for function <function gr_process_files at 0x78614c648ca0>, received 3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://56ef0b65552ebbbcee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://56ef0b65552ebbbcee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvGbyUy6gStZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}